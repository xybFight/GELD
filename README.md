<h1 align="center">From Global Assessment to Local Selction: <br/> Efficiently Solving Traveling Saleman Problems of All Sizes</h1>

A novel TSP solver named GELD (Global-view Encoder and Local-view Decoder) based on the broad global assessment and refined local selection framework

## Dependencies
```bash
Python=3.9.19
torch==2.2.2
numpy==1.23.3
matplotlib==3.8.3
tqdm==4.66.2
pytz==2024.1
```
All packages may not be listed, please install based on the feedback

## Datasets

### Training Dataset
The training datasets "train_TSP100_n100w-001.txt" are downloaded from [LEHD](https://github.com/CIAM-Group/NCO_code/tree/main/single_objective/LEHD). Then place it into the "SL_training_data" folder. Thanks to them for making the dataset publicly available.

### Test Datasets
The test datasets include synthetic and real-world datasets, which are placed in the "Test_data" folder.
#### Synthetic Datasets
The synthetic datasets are randomly generated, and solved by LKH3, following [INVit](https://github.com/Kasumigaoka-Utaha/INViT), also thank to them.
#### Real-worl Dataset
We download 77 instances from [TSPLIB](http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp/) ("tsplib" folder), 27 instances from [National TSPs](https://www.math.uwaterloo.ca/tsp/world/countries.html) ("National_TSP" folder), and 4 extremely large-scale instances from [VLSI data sets](https://www.math.uwaterloo.ca/tsp/vlsi/page11.html) (also "National_TSP" folder)


## Baseline Solution
We use the National TSPs dataset as the benchmark and apply GELD (with PRC1000) to reconstruct the solution generated by baselines. The baseline solutions are placed in "baseline_solutions" folder. 


## Implementation
### Training
We propose a second training strategy. File "train.py" is for the first stage, just excute it to get the SL-based pre-trained model parameters. 
```bash
Python train.py
```

Then, excute "train_new.py" with the correspond path of SL-based model to get the two-stage trained model parameters.
```bash
Python train_new.py
```

### Test
For Synthetic Datasets:
```bash
Python test.py
```
For Real-worl Dataset:
```bash
Python test_inTSPLib.py
```

For Post-processing:
change "from TSPTester_inTSPlib import TSPTester as Tester" to "TSPTester_inTSPlin_post import TSPTester as Tester" in file test_inTSPLib.py, then execute it
```bash
Python test_inTSPLib.py
```



## Acknowledgements
GELD's code implementation is based on the code of [LEHD](https://github.com/CIAM-Group/NCO_code/tree/main/single_objective/LEHD). Thanks to them.

